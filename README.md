# Deployment

FastAPI backend for the multi-agent deployment demo. It exposes endpoints for
creating conversations and streaming responses generated by the OpenAI
Conversations API.

## Requirements

- Python >= 3.13
- uv (Python package manager)

## Setup

1. Install dependencies (creates `.venv` automatically):

   ```bash
   uv sync
   ```

2. Copy `.env.example` to `.env` (or create `.env` manually) and set required
   environment variables:

   ```env
   OPENAI_API_KEY=sk-...
   # Optional: OPENAI_PROJECT=proj-...
   # Optional: comma/semicolon-separated origins allowed to call FastAPI
   ALLOWED_ORIGINS=http://localhost:5173,https://your-frontend.vercel.app
   ```

   The `.env` file is ignored by Git. Rotate leaked keys immediately.

## Running the application

```bash
uv run uvicorn deployment.main:app --reload
```

This command ensures the server runs inside the project `.venv`. To stop the
server, press `Ctrl+C` or run `pkill -f "uvicorn deployment.main:app"`.

## API quickstart

1. Create a conversation:

   ```bash
   curl -X POST http://127.0.0.1:8000/conversations
   ```

2. Ask a question (replace `<conversation_id>` with the ID from step 1):

   ```bash
   curl -N -X POST \
     http://127.0.0.1:8000/conversations/<conversation_id>/message-stream-all \
     -H "Content-Type: application/json" \
     -d '{"question": "What is the size of the Great Wall of China?"}'
   ```

## Development notes

- Python 3.13 and `uv` are required. If `uv run` prints a warning about another
  virtual environment, deactivate it and retry.
- After changing environment variables, restart the server so the new values
  are loaded.
- 기본적으로 FastAPI는 `http://localhost:5173`에서 오는 브라우저 요청을 허용합니다.
  Vercel 등 다른 도메인을 추가하려면 `ALLOWED_ORIGINS`를 설정하세요.

